{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac5eaa32",
   "metadata": {},
   "source": [
    "#  Scaden-methylation Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630a790c",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9935fabf-a638-4de2-9daa-3dde6cb0aa31",
   "metadata": {},
   "source": [
    "Before we can start using the pipeline, we need two datasets:\n",
    "1. A reference dataset to use for training the model. This can be generated in two ways:\n",
    "    - From a real single nucleus methylation dataset (e.g., generated from single-cell bisulfite sequencing). \n",
    "    - From a methylation profile generated from purified samples. This will be used to simulate a single cell dataset.\n",
    "2. A bulk methylation dataset containing the methylation data of samples that we want to perform deconvolution on.\n",
    "\n",
    "There may be other required files, depending on which steps of the pipeline we want to run.\n",
    "Although we focus on methylation data, this program can still work with RNA-seq data like the [original scaden](https://scaden.readthedocs.io/en/latest/usage.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3fa253",
   "metadata": {},
   "source": [
    "## Reference dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f40f3a",
   "metadata": {},
   "source": [
    "### From a real single nucleus methylation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e9fe85",
   "metadata": {},
   "source": [
    "If we want to use a real single cell mathylation dataset, it needs to be converted into text files that follow the format of the original [scaden](https://scaden.readthedocs.io/en/latest/usage.html#bulk-simulation). \n",
    "\n",
    "Our single cell dataset must have two components: the methylation data for the cells, and the labels for each cell.\n",
    "The labels will go into one file with a name that ends in \"`_celltypes.txt`\". This file will have one column called \"Celltype\" with the celltype labels for each cell, e.g.:\n",
    "```\n",
    "Celltype\n",
    "type1\n",
    "type2\n",
    "type9\n",
    "type4\n",
    "```\n",
    "\n",
    "The methylation data will go into another file with a name ending in \"`_counts.txt`\". This file will have a table of methylation values with cell samples as rows and CpGs as columns, e.g.:\n",
    "```\n",
    "     cpg1    cpg2    cpg100  cpg230\n",
    "0    0.0     1.0     0.5     0.0\n",
    "1    1.0     0.25    0.0     0.0\n",
    "2    0.0     0.0     1.0     0.0\n",
    "3    0.75    0.125   0.2     1.0\n",
    "```\n",
    "This table should have row indices that are just numbers starting from 0. The order of the rows should match the order of the cell type labels in the corresponding `_celltypes.txt` file. The two files should also have the same prefix (e.g., `neurons_celltypes.txt` and `neurons_counts.txt` make up one dataset).\n",
    "\n",
    "See the directory `helper_scripts` for some Python scripts you may find useful for generating this dataset from scBS-seq data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07a8c3a9",
   "metadata": {},
   "source": [
    "### From a methylation profile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a557226f-b2f4-49af-bc3c-d9c2b0738275",
   "metadata": {},
   "source": [
    "We can also use a previously created methylation profile generated from purified samples. This consists of a matrix of differentially methylated sites between different cell types. We can sample from this matrix to simulate a single cell dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b0ab9b-ecc8-4b9e-b2ad-0a7ecf155106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBas\tBmem\tBn\tCD4mem\tCD4nv\tTem\tTn\tEOS\tMono\tNeu\tNK\tTreg\n",
      "cg23788058\t0.796\t0.919\t0.94\t0.869\t0.285\t0.897\t0.693\t0.437\t0.949\t0.919\t0.94\t0.9\n",
      "cg21091766\t0.672\t0.688\t0.858\t0.874\t0.936\t0.458\t0.322\t0.583\t0.811\t0.697\t0.739\t0.872\n",
      "cg16596052\t0.886\t0.876\t0.888\t0.833\t0.591\t0.826\t0.116\t0.901\t0.936\t0.914\t0.922\t0.858\n",
      "cg21782213\t0.931\t0.942\t0.945\t0.828\t0.314\t0.935\t0.934\t0.943\t0.953\t0.953\t0.949\t0.919\n",
      "cg15213052\t0.952\t0.947\t0.949\t0.939\t0.891\t0.935\t0.346\t0.96\t0.968\t0.965\t0.841\t0.949\n",
      "cg17458390\t0.073\t0.101\t0.077\t0.068\t0.184\t0.114\t0.692\t0.06\t0.055\t0.051\t0.072\t0.077\n",
      "cg19850895\t0.946\t0.914\t0.94\t0.925\t0.934\t0.869\t0.426\t0.929\t0.96\t0.924\t0.908\t0.946\n",
      "cg27225309\t0.079\t0.922\t0.585\t0.748\t0.266\t0.518\t0.347\t0.071\t0.351\t0.29\t0.186\t0.179\n",
      "cg17518643\t0.941\t0.947\t0.954\t0.581\t0.245\t0.887\t0.791\t0.583\t0.929\t0.909\t0.91\t0.719\n",
      "cg12043508\t0.79\t0.771\t0.881\t0.909\t0.916\t0.881\t0.453\t0.833\t0.823\t0.802\t0.724\t0.923\n",
      "cg20813776\t0.94\t0.925\t0.931\t0.89\t0.937\t0.386\t0.91\t0.929\t0.937\t0.923\t0.848\t0.924\n",
      "cg00267793\t0.149\t0.927\t0.921\t0.916\t0.92\t0.921\t0.92\t0.921\t0.939\t0.937\t0.933\t0.9\n",
      "cg17491554\t0.918\t0.061\t0.849\t0.793\t0.943\t0.772\t0.941\t0.946\t0.941\t0.948\t0.903\t0.812\n",
      "cg02053685\t0.893\t0.865\t0.875\t0.848\t0.816\t0.907\t0.477\t0.936\t0.933\t0.924\t0.726\t0.936\n"
     ]
    }
   ],
   "source": [
    "# Example reference library for peripheral blood\n",
    "!head -n 15 example/blood_matrix.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "759ca2e8-2176-446e-86e6-48c047cc8db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created files 'example/data_counts.txt' and 'example/data_celltypes.txt'\n"
     ]
    }
   ],
   "source": [
    "# Sample from the reference matrix to create a simulated single cell dataset\n",
    "!python helper_scripts/array_to_sc.py -s 100 -r 2 -o 'example/data' example/blood_matrix.tsv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc004cbc-73ee-4c41-81a3-4fb938db9dea",
   "metadata": {},
   "source": [
    "This script samples each value in the table from a binomial distribution with 1 trial and uses the value as the probability.\n",
    "The number of successes (0 or 1) is used as the corresponding value in the scMethyl-seq dataset.\n",
    "The arguments used are:\n",
    "- `-s`: Numpy random generator seed for reproducibility.\n",
    "- `-r`: The number of simulated cells to generate for each cell type in the matrix. \n",
    "    - By setting this to 2, we created a dataset with 24 cells (2 * 12 cell types). \n",
    "    - Higher values (e.g.: 1000) generally give better results.\n",
    "- `-o`: Set the name and location of the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a237a15-5def-49a9-9b3e-cf51091bc085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Celltype\n",
      "Bas\n",
      "Bas\n",
      "Bmem\n",
      "Bmem\n",
      "Bn\n",
      "Bn\n",
      "CD4mem\n",
      "CD4mem\n",
      "CD4nv\n",
      "CD4nv\n",
      "Tem\n",
      "Tem\n",
      "Tn\n",
      "Tn\n",
      "EOS\n",
      "EOS\n",
      "Mono\n",
      "Mono\n",
      "Neu\n",
      "Neu\n",
      "NK\n",
      "NK\n",
      "Treg\n",
      "Treg\n"
     ]
    }
   ],
   "source": [
    "# Celltype labels file. Each cell type was sampled twice.\n",
    "!cat example/data_celltypes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8f27038-2f76-4b69-b74d-e1015fab537e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcg23788058\tcg21091766\tcg16596052\tcg21782213\tcg15213052\tcg17458390\tcg19850895\n",
      "0\t0\t0\t1\t1\t1\t0\t1\n",
      "1\t1\t0\t1\t1\t1\t0\t1\n",
      "2\t1\t0\t1\t1\t1\t0\t1\n",
      "3\t1\t1\t1\t1\t0\t1\t1\n",
      "4\t0\t1\t1\t1\t1\t0\t1\n",
      "5\t1\t1\t1\t1\t1\t0\t1\n",
      "6\t1\t1\t1\t1\t1\t0\t1\n",
      "7\t0\t1\t1\t1\t1\t0\t1\n",
      "8\t0\t1\t0\t1\t1\t0\t1\n",
      "9\t0\t1\t1\t0\t1\t0\t1\n",
      "10\t0\t0\t1\t1\t1\t0\t0\n",
      "11\t1\t1\t1\t1\t1\t0\t1\n",
      "12\t1\t0\t0\t1\t1\t1\t0\n",
      "13\t1\t0\t0\t1\t1\t1\t1\n",
      "14\t1\t1\t1\t1\t1\t0\t1\n",
      "15\t0\t1\t1\t1\t1\t0\t1\n",
      "16\t0\t0\t1\t1\t1\t1\t1\n",
      "17\t1\t1\t1\t1\t1\t0\t1\n",
      "18\t1\t1\t1\t0\t1\t0\t1\n",
      "19\t1\t0\t1\t1\t1\t0\t1\n",
      "20\t0\t1\t1\t1\t1\t0\t1\n",
      "21\t0\t1\t1\t1\t0\t0\t1\n",
      "22\t1\t1\t1\t0\t0\t0\t1\n",
      "23\t1\t0\t1\t1\t1\t0\t1\n"
     ]
    }
   ],
   "source": [
    "# First 8 columns of our simulated methylation data\n",
    "!cut -f 1-8 example/data_counts.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3da7b4aa-3af6-44b6-982d-a996816aebfc",
   "metadata": {},
   "source": [
    "## Using the pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0de59279-d825-452d-b4b8-95d0dd2c9487",
   "metadata": {},
   "source": [
    "There are 5 steps in the pipeline:\n",
    "1. Simulate training data from single cell dataset\n",
    "2. Preprocess training data\n",
    "3. Train model\n",
    "4. Predict on testing data\n",
    "5. Evaluate predictions\n",
    "\n",
    "Each step has its own set of parameters that will be explained below with an example run. \n",
    "To run the pipeline:\n",
    "- Save the parameters to a YAML file, then run `python main.py --load params.yaml` (Recommended)\n",
    "- Run `python main.py ...` with all the parameters as a single command."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e073711b-33ec-4c30-861e-def3e833515c",
   "metadata": {},
   "source": [
    "### Pipeline controls and logging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8cecad8-e9c3-49eb-8439-b5dc0aa4835e",
   "metadata": {},
   "source": [
    "```\n",
    "  -load TEXT                 YAML file from which parameters are loaded\n",
    "  -v, --verify               Verify that parameters are valid, then exit\n",
    "                             without doing any steps.\n",
    "  -all                       Run all steps of the pipeline (simulate, process,\n",
    "                             train, predict, evaluate)\n",
    "  -simulate                  Run scaden simulate\n",
    "  -process                   Run scaden process\n",
    "  -train                     Run scaden train\n",
    "  -predict                   Run scaden predict\n",
    "  -evaluate                  Run evaluation\n",
    "  --config TEXT              Name of configuration  [default: test]\n",
    "  --reference TEXT           Name of the dataset  [default: None]\n",
    "  --log_params               Create a json file recording the data and model\n",
    "                             hyperparameters\n",
    "  --seed INTEGER             Set random seed  [default: 0]\n",
    "```\n",
    "\n",
    "For this example, we will go through all steps of the pipeline, so we can just use the `-all` flag. If we only want to run specific steps add each flag (e.g., `-predict -evaluate`) \n",
    "\n",
    "We can set some parameters for logging purposes. These parameters will appear in the output logs:\n",
    "- `--config example`\n",
    "- `--reference blood`\n",
    "  - The name of our configuration and our dataset, respectively.\n",
    "- `--seed 100`\n",
    "  - The seed that will be used to initialize the NumPy random number generator."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bee5dba-1b14-49e1-a854-cdfb5e2a4a82",
   "metadata": {},
   "source": [
    "### Simulate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2061eeec-fd97-42fe-b880-491a6838ebc5",
   "metadata": {},
   "source": [
    "This step will sample from the dataset files and create a h5ad file containing the simualted bulk samples and the ground truth fractions. \n",
    "\n",
    "If there is more than one dataset in the `data` directory (i.e., more than one set of x_counts.txt and x_celltypes.txt files), scaden will create a h5ad file for each dataset and a file that combines the datasets.\n",
    "\n",
    "```\n",
    "  -o, --out TEXT             Directory to store output files in  [default: ./]\n",
    "  -d, --data TEXT            Path to scRNA-seq dataset(s)  [default: .]\n",
    "  -c, --cells INTEGER        Number of cells per sample  [default: 100]\n",
    "  -n, --n_samples INTEGER    Number of samples to simulate  [default: 1000]\n",
    "  --pattern TEXT             File pattern to recognize your processed scRNA-\n",
    "                             seq count files  [default: *_counts.txt]\n",
    "  -u, --unknown TEXT         Specify cell types to merge into the unknown\n",
    "                             category. Specify this flag for every cell type\n",
    "                             you want to merge in unknown.  [default: unknown]\n",
    "  -p, --prefix TEXT          Prefix to append to training .h5ad file\n",
    "                             [default: data]\n",
    "  -f, --data_format TEXT     Data format of scRNA-seq data, can be 'txt' or\n",
    "                             'h5ad'  [default: txt]\n",
    "\n",
    "```\n",
    "\n",
    "Here's what we'll use:\n",
    "- `--data example/`\n",
    "  - We previously created our single cell dataset and stored it in the 'example' directory, so that will be our data directory.\n",
    "- `--out example/`\n",
    "  - We'll store our simulated training data in the same directory.\n",
    "- `--cells 100 -n 1000`\n",
    "  - The training set will contain 1000 samples, each composed of 100 cells.\n",
    "- `--prefix example_data`\n",
    "  - The training set will be named 'example_data'.\n",
    "\n",
    "The result is a file called '`example_data.h5ad`' in our `example/` directory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd581e4b-deef-4ad1-afd8-5ba8a2637489",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef2ece58-ddb6-4018-8c86-8885936a1e29",
   "metadata": {},
   "source": [
    "This step will take the training data from the previous step and do three things:\n",
    "1. Filter the dataset for CpGs that are found in both the training data and the data we are performing deconvolution on.\n",
    "2. Remove CpGs that have a variance below a specified cutoff.\n",
    "3. Apply a scaling to the methylation data.\n",
    "\n",
    "The testing data should be a text file or h5ad file with CpGs in the rows and samples in the columns, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3844a59f-cc2e-4af1-b457-d30f7a7796eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSM5527895\tGSM5527896\tGSM5527897\tGSM5527898\tGSM5527899\tGSM5527900\tGSM5527901\tGSM5527902\n",
      "cg23788058\t0.798699449\t0.784677456\t0.74981309\t0.789639854\t0.810941974\t0.825244463\t0.788397419\n",
      "cg21091766\t0.687263871\t0.730978837\t0.743193841\t0.645360376\t0.69560332\t0.72999294\t0.737264397\n",
      "cg16596052\t0.879610284\t0.833583581\t0.841797798\t0.735548403\t0.830872763\t0.844413481\t0.76839483\n",
      "cg21782213\t0.868848813\t0.878711025\t0.833995666\t0.862394569\t0.877224017\t0.882607419\t0.846306483\n",
      "cg15213052\t0.889969575\t0.887590506\t0.885214628\t0.864699342\t0.865485621\t0.882226631\t0.884852479\n",
      "cg17458390\t0.117161441\t0.080060866\t0.071458516\t0.143326585\t0.08785156\t0.074913869\t0.090899878\n",
      "cg19850895\t0.878980332\t0.875194166\t0.902452525\t0.805203742\t0.875195158\t0.873695376\t0.844872471\n",
      "cg27225309\t0.438220517\t0.302231866\t0.421263644\t0.335391233\t0.251055594\t0.336748842\t0.322993831\n",
      "cg17518643\t0.817972414\t0.76323919\t0.700720148\t0.810156068\t0.819491989\t0.796900676\t0.79493788\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 example/testfile.tsv | cut -f 1-8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e585bcc2-6607-4ef6-bd19-0f8dcc2cc729",
   "metadata": {},
   "source": [
    "Now we'll set the parameters, which are the following:\n",
    "\n",
    "```\n",
    "  --pred TEXT                Bulk data file (or testing set) that we want to\n",
    "                             perform deconvolution on. Should be a text or\n",
    "                             h5ad file with rows as genes and samples as\n",
    "                             columns.\n",
    "  --training_data TEXT       Training dataset to be processed. Only use this\n",
    "                             if you are running 'scaden process' by itself.\n",
    "  --processed_path TEXT      Name of the file that the processed data will be\n",
    "                             saved to. Must end with .h5ad\n",
    "  --var_cutoff FLOAT         Filter out genes with a variance less than the\n",
    "                             specified cutoff. A low value is recommended,\n",
    "                             this should only remove genes that are obviously\n",
    "                             uninformative.  [default: 0.1]\n",
    "  --scaling TEXT             Change scaling option for preprocessing the\n",
    "                             training data. If something other than the\n",
    "                             provided options is used, then no scaling will be\n",
    "                             done. Options: None (No scaling), log /\n",
    "                             log_min_max (log2, then scale to the range 0,1),\n",
    "                             frac / fraction (Divide values by the number of\n",
    "                             cells), frac_notna (Divide values by the number\n",
    "                             of non-NA values. This must be used with the\n",
    "                             simulation step and cannot be done by\n",
    "                             preprocessing separately)  [default: fraction]\n",
    "\n",
    "```\n",
    "\n",
    "We will use these parameters:\n",
    "- `--pred example/testfile.tsv`\n",
    "    - This file contains the data for the bulk samples that we want to perform deconvolution on.\n",
    "- `--scaling frac`\n",
    "    - Fraction scaling, since we are dealing with methylation data of simulated cells\n",
    "- `--var_cutoff 0`\n",
    "    - We will assume that the CpGs in our reference methylation array are all important, so we don't want to remove any of them.\n",
    "\n",
    "\n",
    "The scaling option has 4 options:\n",
    "- None\n",
    "- log / log_min_max: Apply log2, then scale to the range 0,1. \n",
    "    - This is the default option for RNA-seq data.\n",
    "- frac / fraction: Divide values by the total number of cells. \n",
    "    - This should be used with methyl-seq if our single-cell dataset was simulated from a methylation profile.\n",
    "- frac_notna: Divide values by the number of non-NA values.\n",
    "    - This should be used with methyl-seq if we're using a real single-cell dataset with many missing values.\n",
    "\n",
    "Some parameters are unnecessary or automatically handled if we are going through the entire pipeline. \n",
    "- `--training_data`: The h5ad file generated by the simulation step. If we are running the preprocessing separately from the simulation, we need to set this parameter.\n",
    "- `--processed_path`: Set this if we want to change the name or location of the preprocessed training data By default, the output will be named '`processed.h5ad`' and be in the directory specified with `--out`.\n",
    "\n",
    "\n",
    "The result is a file (named '`processed.h5ad`' by default) that contains our preprocessed data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c200335f-94e3-4f64-a58f-52f18bb168f3",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6879db4e-db68-48f5-b974-28376d73d5a6",
   "metadata": {},
   "source": [
    "This step is where the model training is done.\n",
    "\n",
    "```\n",
    "  --train_datasets TEXT      Comma-separated list of datasets used for\n",
    "                             training. Uses all by default.\n",
    "  --model_dir TEXT           Path to store the model in  [default: ./]\n",
    "  --batch_size INTEGER       Batch size to use for training.  [default: 128]\n",
    "  --learning_rate FLOAT      Learning rate used for training.  [default:\n",
    "                             0.0001]\n",
    "  --steps INTEGER            Number of training steps.  [default: 5000]\n",
    "  --loss_values TEXT         Name of file to save text file of loss values\n",
    "  --loss_curve TEXT          Name of file to save line plot figure of loss\n",
    "                             values\n",
    "```\n",
    "\n",
    "We will use the parameters:\n",
    "- `--model_dir example/model/`\n",
    "  - Our model will be stored in the specified directory. This directory must exist before running the training.\n",
    "- `--batch_size 256`\n",
    "- `--learning_rate 0.0001`\n",
    "- `--steps 1000`\n",
    "  - Various model training hyperparameters.\n",
    "- `--loss_values example/loss_values.tsv`\n",
    "  - (OPTIONAL) This will create a text file containing the loss values after each step of training.\n",
    "- `--loss_curve example/loss_curve.png`\n",
    "  - (OPTIONAL) This will create a simple plot for the loss values vs step.\n",
    "\n",
    "If we are running this step separately, we need to specify the following:\n",
    "- `--processed_path`: Preprocessed training data\n",
    "\n",
    "The result the creation of a set of files in our model directory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5d71e3d-44bf-47d7-a762-d4175ac10ed8",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c513ad8-2351-470b-969c-10335a19a930",
   "metadata": {},
   "source": [
    "This step will use a trained model to create predictions on bulk samples to predict the cell type proportions.\n",
    "\n",
    "```\n",
    "  --prediction_outname TEXT  Name of predictions file  [default:\n",
    "                             scaden_predictions.txt]\n",
    "  --prediction_scaling TEXT  Change scaling option for the preprocessing done\n",
    "                             when making predictions. Uses the same options as\n",
    "                             --scaling.  [default: fraction]\n",
    "```\n",
    "\n",
    "Our parameters:\n",
    "- `--prediction_outname example/test_predictions.txt`\n",
    "  - Name of the output text file.\n",
    "- `--prediction_scaling None`\n",
    "  - Scaling to apply to the testing data. If this data is a real methylation dataset, don't use any scaling.\n",
    "\n",
    "If we are running this step separately, we need to specify the following:\n",
    "- `--model_dir`: Directory to the trained model.\n",
    "- `--pred`: Bulk samples to perform deconvolution on.\n",
    "- `--cells`: If we want to use fraction scaling.\n",
    "\n",
    "The result is a text file containing the predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bffc25a-b439-4c74-8bf0-19f4f4814f27",
   "metadata": {},
   "source": [
    "## Evaluate (WIP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7fb828f-db04-4e73-87ae-0850ded3f69d",
   "metadata": {},
   "source": [
    "This step will compare the predictions with the ground truth by calculating the correlation coefficients and mean squared errors.\n",
    "\n",
    "```\n",
    "  --ground_truth TEXT        Name of file containing the ground truth cell\n",
    "                             proportions\n",
    "```\n",
    "\n",
    "- `--ground_truth example/test_proportions.tsv`\n",
    "  - The ground truth proportions should be a table where the columns are cell types and the rows are samples:\n",
    "\n",
    "The result is a JSON file containing the results of the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1a589fe-c57e-42e8-a47e-68b71894d67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bas\tbmem\tbnv\tcd4mem\tcd4nv\tcd8mem\tcd8nv\teos\tmono\tneu\tnk\ttreg\n",
      "GSM5527895\t13.14\t13.26\t6.53\t10.08\t4.47\t10.95\t5.99\t9.01\t6.89\t3.02\t6.36\t10.28\n",
      "GSM5527896\t11.54\t8.6\t6.71\t8.65\t8.14\t9.13\t6.16\t8.62\t3.72\t8.63\t8.43\t11.67\n",
      "GSM5527897\t8.75\t11.12\t3.68\t21.32\t10.16\t11.84\t2.8\t5.05\t3.92\t3.57\t5.14\t12.68\n",
      "GSM5527898\t8.53\t7.2\t18.76\t4.75\t7.9\t9.76\t14.75\t7.05\t5.78\t3.92\t7.35\t4.24\n",
      "GSM5527899\t6.98\t5.22\t5.16\t6.04\t7.61\t8.29\t8.19\t5.12\t8.09\t8.28\t22.23\t8.79\n",
      "GSM5527900\t9.42\t12.49\t7.14\t8.11\t5.07\t4.9\t6.82\t10.43\t4.96\t15.74\t9.71\t5.21\n",
      "GSM5527901\t2.29\t7.85\t6.46\t8.54\t9.55\t9.55\t9.99\t11.26\t13.28\t9.18\t4.52\t7.51\n",
      "GSM5527902\t5.56\t5.37\t7.58\t6.23\t4.18\t5.06\t3.83\t7.97\t10.63\t15.69\t16.92\t10.97\n",
      "GSM5527903\t7.39\t7.84\t11.17\t14.13\t8.38\t6.5\t1.9\t10.29\t6.38\t13.74\t8.2\t4.08\n"
     ]
    }
   ],
   "source": [
    "!head example/test_proportions.tsv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a965991-6400-42dd-b284-b9578196235d",
   "metadata": {},
   "source": [
    "## Putting everything together"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd31b16d",
   "metadata": {},
   "source": [
    "YAML file `params.yaml':\n",
    "```\n",
    "all: True\n",
    "config: example\n",
    "reference: blood\n",
    "seed: 100\n",
    "data: example/\n",
    "out: example/\n",
    "cells: 100\n",
    "n_samples: 1000\n",
    "prefix: example_data\n",
    "pred: example/testfile.tsv\n",
    "scaling: frac\n",
    "var_cutoff: 0\n",
    "model_dir: example/model/\n",
    "batch_size: 256\n",
    "learning_rate: 0.0001\n",
    "steps: 1000\n",
    "loss_values: example/loss_values.tsv\n",
    "loss_curve: example/loss_curve.png\n",
    "prediction_outname: example/test_predictions.txt\n",
    "prediction_scaling: None\n",
    "ground_truth: example/test_proportions.tsv\n",
    "```\n",
    "\n",
    "Then we run the program with these parameters using: `python main.py -load params.yaml`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab7d8706-9f5e-4a08-b720-c898627757ff",
   "metadata": {},
   "source": [
    "## Example Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b47d775-5aa8-4b60-b4e3-2e0440c81926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "     ____                _            \n",
      "    / ___|  ___ __ _  __| | ___ _ __  \n",
      "    \\___ \\ / __/ _` |/ _` |/ _ \\ '_ \\ \n",
      "     ___) | (_| (_| | (_| |  __/ | | |\n",
      "    |____/ \\___\\__,_|\\__,_|\\___|_| |_|\n",
      "    \u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m Using params from: example/params.yaml                      \u001b[2mmain.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m112\u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m Datasets: \u001b[1;36m[\u001b[0m\u001b[32m'data'\u001b[0m\u001b[1;36m]\u001b[0m                                 \u001b[2mbulk_simulator.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m99\u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m \u001b[1;4mSimulating data from data\u001b[0m                         \u001b[2mbulk_simulator.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m105\u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m Loading \u001b[36mdata\u001b[0m dataset \u001b[33m...\u001b[0m                          \u001b[2mbulk_simulator.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m144\u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m Merging unknown cell types: \u001b[1m[\u001b[0m\u001b[32m'unknown'\u001b[0m\u001b[1m]\u001b[0m           \u001b[2mbulk_simulator.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m226\u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m Subsampling \u001b[1;36mdata\u001b[0m \u001b[33m...\u001b[0m                              \u001b[2mbulk_simulator.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m229\u001b[0m\n",
      "\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m0\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m21\u001b[0m \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m44\u001b[0m \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m68\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m93\u001b[0m \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m117\u001b[0m \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m141\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m164\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m187\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m211\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m235\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m259\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m282\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m304\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m324\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m346\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m369\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m392\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m415\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m435\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m455\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m475\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m494\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m500\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m500\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m500\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m500\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m500\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m500\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m500\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m500\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m500\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m500\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m500\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m500\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m500\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m500\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m━━\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1;34mNormal samples\u001b[0m \u001b[1;36m500\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m━━\u001b[0m\n",
      "\u001b[1;34mSparse samples\u001b[0m \u001b[1;36m500\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n",
      "\u001b[?25h\u001b[34mINFO    \u001b[0m \u001b[1;32mFinished data simulation!\u001b[0m                         \u001b[2mbulk_simulator.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m108\u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m Merging datasets: \u001b[1m[\u001b[0m\u001b[32m'example/data.h5ad'\u001b[0m\u001b[1m]\u001b[0m into      \u001b[2mbulk_simulator.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m422\u001b[0m\n",
      "         \u001b[1;36mexample_data.h5ad\u001b[0m                                 \u001b[2m                     \u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m Pre-processing raw data \u001b[33m...\u001b[0m                               \u001b[2mprocess.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m59\u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m Subsetting genes \u001b[33m...\u001b[0m                                      \u001b[2mprocess.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m63\u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m Scaling using frac                                        \u001b[2mprocess.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m66\u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m Writing to disk \u001b[33m...\u001b[0m                                       \u001b[2mprocess.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m74\u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m Data pre-processing done.                                 \u001b[2mprocess.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m76\u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m Created processed file: \u001b[36mexample/processed.h5ad\u001b[0m            \u001b[2mprocess.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m77\u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m \u001b[36mTraining M256 Model \u001b[0m\u001b[33m...\u001b[0m\u001b[36m \u001b[0m                                    \u001b[2mtrain.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m54\u001b[0m\n",
      "\u001b[2K\u001b[1;34mm256\u001b[0m \u001b[1;36mStep: 999, Loss: 0.0006\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m[91m╸\u001b[0m[90m━\u001b[0m\n",
      "\u001b[?25h\u001b[31mWARNING \u001b[0m Compiled the loaded model, but the compiled metrics \u001b[2msaving_utils.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m359\u001b[0m\n",
      "         have yet to be built. `model.compile_metrics` will  \u001b[2m                   \u001b[0m\n",
      "         be empty until you train or evaluate the model.     \u001b[2m                   \u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m Assets written to: example/model/\u001b[35m/m256/\u001b[0m\u001b[95massets\u001b[0m       \u001b[2mbuilder_impl.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m797\u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m \u001b[36mTraining M512 Model \u001b[0m\u001b[33m...\u001b[0m\u001b[36m \u001b[0m                                    \u001b[2mtrain.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m70\u001b[0m\n",
      "\u001b[2K\u001b[1;34mm512\u001b[0m \u001b[1;36mStep: 999, Loss: 0.0016\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m[91m╸\u001b[0m[90m━\u001b[0m\n",
      "\u001b[?25h\u001b[31mWARNING \u001b[0m Compiled the loaded model, but the compiled metrics \u001b[2msaving_utils.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m359\u001b[0m\n",
      "         have yet to be built. `model.compile_metrics` will  \u001b[2m                   \u001b[0m\n",
      "         be empty until you train or evaluate the model.     \u001b[2m                   \u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m Assets written to: example/model/\u001b[35m/m512/\u001b[0m\u001b[95massets\u001b[0m       \u001b[2mbuilder_impl.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m797\u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m \u001b[36mTraining M1024 Model \u001b[0m\u001b[33m...\u001b[0m\u001b[36m \u001b[0m                                   \u001b[2mtrain.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m86\u001b[0m\n",
      "\u001b[2K\u001b[1;34mm1024\u001b[0m \u001b[1;36mStep: 999, Loss: 0.0016\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m[91m╸\u001b[0m[90m━\u001b[0m\n",
      "\u001b[?25h\u001b[31mWARNING \u001b[0m Compiled the loaded model, but the compiled metrics \u001b[2msaving_utils.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m359\u001b[0m\n",
      "         have yet to be built. `model.compile_metrics` will  \u001b[2m                   \u001b[0m\n",
      "         be empty until you train or evaluate the model.     \u001b[2m                   \u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m Assets written to: example/model/\u001b[35m/m1024/\u001b[0m\u001b[95massets\u001b[0m      \u001b[2mbuilder_impl.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m797\u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m \u001b[32mTraining finished.\u001b[0m                                         \u001b[2mtrain.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m101\u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m Loss values written to: example/loss_values.tsv            \u001b[2mtrain.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m109\u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m Loss curve saved to: example/loss_curve.png                \u001b[2mtrain.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m115\u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m \u001b[1mTraining time: \u001b[0m\u001b[1;36m68.637\u001b[0m\u001b[1;32m seconds\u001b[0m                               \u001b[2mmain.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m253\u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m Loaded pre-trained model: \u001b[36mm256\u001b[0m                      \u001b[2mclass_scaden.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m326\u001b[0m\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "\u001b[34mINFO    \u001b[0m Loaded pre-trained model: \u001b[36mm512\u001b[0m                      \u001b[2mclass_scaden.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m326\u001b[0m\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "\u001b[34mINFO    \u001b[0m Loaded pre-trained model: \u001b[36mm1024\u001b[0m                     \u001b[2mclass_scaden.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m326\u001b[0m\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "\u001b[34mINFO    \u001b[0m \u001b[1mCreated cell composition predictions: \u001b[0m                    \u001b[2mpredict.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m98\u001b[0m\n",
      "         \u001b[1;32mexample/test_predictions.txt\u001b[0m                              \u001b[2m             \u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m Evaluating predictions \u001b[33m...\u001b[0m                               \u001b[2mevaluate.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m90\u001b[0m\n",
      "\u001b[34mINFO    \u001b[0m \u001b[1mCreated evaluation results: \u001b[0m\u001b[1;32mexample/report_example.json\u001b[0m \u001b[2mevaluate.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m104\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python main.py -load example/params.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48265c19",
   "metadata": {},
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfcbdf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tBas\tBmem\tBn\tCD4mem\tCD4nv\tTem\tTn\tEOS\tMono\tNeu\tNK\tTreg\n",
      "GSM5527895\t0.119815744\t0.119674735\t0.07861956\t0.07423564\t0.06338039\t0.082098626\t0.07961836\t0.075796954\t0.06651795\t0.06706738\t0.07943547\t0.093739145\n",
      "GSM5527896\t0.11470125\t0.09604945\t0.080230914\t0.082268655\t0.06662912\t0.076183975\t0.08212555\t0.08213074\t0.06365629\t0.07871834\t0.080222696\t0.09708298\n",
      "GSM5527897\t0.10378856\t0.1040929\t0.07022439\t0.10760345\t0.06498163\t0.09262127\t0.08279188\t0.063097745\t0.057208937\t0.06470912\t0.06934353\t0.11953661\n",
      "GSM5527898\t0.10580767\t0.09764365\t0.11450633\t0.06978834\t0.07349116\t0.067969695\t0.10282135\t0.06938842\t0.07297294\t0.06909428\t0.081032775\t0.07548339\n",
      "GSM5527899\t0.10259307\t0.08286381\t0.0727314\t0.0781588\t0.06294493\t0.0780484\t0.093173645\t0.06892962\t0.075281166\t0.08531227\t0.111167096\t0.08879581\n",
      "GSM5527900\t0.10923013\t0.10604584\t0.08387625\t0.06774113\t0.063241765\t0.06708991\t0.07720655\t0.08491834\t0.07870664\t0.102051705\t0.08264004\t0.07725167\n",
      "GSM5527901\t0.09278202\t0.09506547\t0.079252005\t0.07149267\t0.07475855\t0.068644494\t0.09218053\t0.077421546\t0.09628397\t0.094278\t0.070763476\t0.08707729\n",
      "GSM5527902\t0.09895772\t0.08763795\t0.07696582\t0.07285882\t0.058655333\t0.07234613\t0.07638723\t0.07819749\t0.0996033\t0.098859735\t0.09557251\t0.08395789\n",
      "GSM5527903\t0.10605008\t0.09720501\t0.08960316\t0.08119825\t0.060641557\t0.067084745\t0.07684156\t0.086718954\t0.07965324\t0.09783562\t0.07591197\t0.08125582\n",
      "GSM5527904\t0.09828738\t0.09379528\t0.081421934\t0.076835334\t0.07719541\t0.06968585\t0.09448751\t0.067836165\t0.0944438\t0.08606825\t0.070720084\t0.08922291\n",
      "GSM5527905\t0.080257095\t0.09267556\t0.09149289\t0.0894188\t0.06963774\t0.08421057\t0.09457222\t0.06387519\t0.07420632\t0.08513408\t0.08133557\t0.09318399\n",
      "GSM5527906\t0.088259816\t0.09269798\t0.09013929\t0.063740246\t0.07272554\t0.066808015\t0.09440059\t0.095232606\t0.08597203\t0.101770155\t0.07485477\t0.07339898\n"
     ]
    }
   ],
   "source": [
    "!cat example/test_predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "967a266a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"training_time\": 68.637,\n",
      "    \"results\": {\n",
      "        \"Predictions best correlated celltype\": {\n",
      "            \"bas\": \"Bas\",\n",
      "            \"bmem\": \"Bmem\",\n",
      "            \"bnv\": \"Bn\",\n",
      "            \"cd4mem\": \"CD4mem\",\n",
      "            \"cd4nv\": \"CD4nv\",\n",
      "            \"cd8mem\": \"Tem\",\n",
      "            \"cd8nv\": \"Tn\",\n",
      "            \"eos\": \"EOS\",\n",
      "            \"mono\": \"Mono\",\n",
      "            \"neu\": \"Neu\",\n",
      "            \"nk\": \"NK\",\n",
      "            \"treg\": \"Treg\"\n",
      "        },\n",
      "        \"Correlation coefficient\": {\n",
      "            \"bas\": 0.9784099842250115,\n",
      "            \"bmem\": 0.9186740709042494,\n",
      "            \"bnv\": 0.9694224444786744,\n",
      "            \"cd4mem\": 0.8976052143901749,\n",
      "            \"cd4nv\": 0.6680259811738782,\n",
      "            \"cd8mem\": 0.6295658025141089,\n",
      "            \"cd8nv\": 0.8458940221241743,\n",
      "            \"eos\": 0.8844145633825922,\n",
      "            \"mono\": 0.8822245844210457,\n",
      "            \"neu\": 0.92459867657708,\n",
      "            \"nk\": 0.9716180918400404,\n",
      "            \"treg\": 0.749339548692836\n",
      "        },\n",
      "        \"Mean squared error\": {\n",
      "            \"bas\": 0.002201282053550332,\n",
      "            \"bmem\": 0.0005193911183276918,\n",
      "            \"bnv\": 0.000856448469114767,\n",
      "            \"cd4mem\": 0.0016423202686110206,\n",
      "            \"cd4nv\": 0.0007032754522213686,\n",
      "            \"cd8mem\": 0.0007748337523232285,\n",
      "            \"cd8nv\": 0.0011232471008380972,\n",
      "            \"eos\": 0.0005757454665664107,\n",
      "            \"mono\": 0.00033678090432119417,\n",
      "            \"neu\": 0.0009506276132502423,\n",
      "            \"nk\": 0.001673733366185411,\n",
      "            \"treg\": 0.0006600636973235524\n",
      "        }\n",
      "    }\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat example/report_example.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d758133-5ee9-4899-a172-6088654ef795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step\tmodel\tloss\n",
      "0\tm256\t0.0117640420794487\n",
      "1\tm256\t0.009992626495659351\n",
      "2\tm256\t0.01273795310407877\n",
      "3\tm256\t0.009805307723581791\n",
      "4\tm256\t0.011787560768425465\n",
      "5\tm256\t0.010207257233560085\n",
      "6\tm256\t0.010412284173071384\n",
      "7\tm256\t0.01030189823359251\n",
      "8\tm256\t0.010286832228302956\n"
     ]
    }
   ],
   "source": [
    "!head example/loss_values.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bac66c4-2da6-43a6-b82b-77ba4535b7a7",
   "metadata": {},
   "source": [
    "**Loss curve:** \n",
    "\n",
    "![Loss curve](example/loss_curve.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ecae113",
   "metadata": {},
   "source": [
    "## Required parameters for each step"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7160ee47",
   "metadata": {},
   "source": [
    "These arguments must be provided if using individual commands.\n",
    "\n",
    "Simulate\n",
    "- `out`: Directory to store output files in.\n",
    "- `data`: Path to single-cell dataset(s)\n",
    "\n",
    "Process\n",
    "- `pred`: Bulk data file that we want to perform deconvolution on.\n",
    "- `training_data`: Training dataset to be processed, generated by the simulate step.\n",
    "- `processed_path`: Name of the file that the processed data will be saved to. Must end with .h5ad\n",
    "\n",
    "Train\n",
    "- `processed_path`: Name of file containing the data that will be used for training.\n",
    "- `model_dir`: Path to store trained model.\n",
    "\n",
    "Predict\n",
    "- `pred`: Bulk data file that we want to perform deconvolution on.\n",
    "- `model_dir`: Path to trained model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
